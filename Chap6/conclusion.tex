In this thesis, I presented methods for learning and planning with dynamics models of deformable one-dimensional objects (DOOs). By developing data augmentation and focused adaptation methods, we achieve the data efficiency needed to learn models on real robots while they work to complete useful manipulation tasks, in fairly narrow settings. The models are not accurate everywhere, but our learning and planning methods are aware of this and account for it. Ultimately, these methods enabled the robot to perform parts of tasks like installing a hose in the hood of a car, plugging in USB or extension cables, or use a vacuum. This work advances the state of the art in robotic manipulation of DOOs, but there are several limitations and exciting directions for future work.

First, I have assumed that the state of the rope is be accurately tracked at all times during the manipulation, and our experiments were set up carefully to allow for this. However, heavy occlusion, brittle calibration methods, fast motion of the rope, and tangling of the rope all make accurate tracking difficult. By requiring accurate perception, we are severely limiting the tasks the robot can do. Therefore, we need to relax our assumption of accurate perception. This could be accomplished by adding per-point uncertainty estimates to the rope state representation, or by using partial-shape or higher level topological state when the full shape is unknown. For example, for the task of plugging in the cable, we care most about where the top of the cable is, and should not necessarily need to acurately track the entire cable. Additionally, planning and control methods may need to account for this uncertainty (for which there are many existing methods \cite{platt2010belief, FIRM, LQGMP}). Alternatively, one could use state representations learned directly from images, instead of those designed by hand \cite{Hafner2019}.

Related to this is our dependence on scene cameras. In all our experiments, multiple scene cameras were placed around the (stationary) robot to minimize occlusion. This also requires calibration of these cameras with respect to the robot. This process seems ill-suited for robots in the wild, especially robots doing mobile manipulation. Even in semi-controlled environments such as warehouses, relying on scene cameras will limit the flexibility and robustness of the system. Instead, the robot should be equipped with multiple on-board cameras, and should actively move those cameras to see the objects it is manipulating or obstacles it is avoiding.

Beyond cameras and perception, we are also presently limited by the hardware of our robot arms. Notably, the robot Val used in almost all the experiments in this thesis has significant backlash which cannot be sensed by the joint encoders. It also lacks dexterous hands, tactile sensing, or compliant controllers. In my opinion, the ideal robot for DOO manipulation would have compliant controllers, dexterous and sensorized grippers, two arms with 2-3 torso joints, and a mobile base.

Finally, we are limited by learning methods that are specialized to certain data types, or observation/state/action spaces. For instance, the method I proposed use joint configurations, points representing the DOO, and voxelized environment geometry, but even within my own methods there are differences in data types. There are also other methods that work well on large datasets of RGB images and end-effector pose actions \cite{RT2}. While we have methods that work well in some cases, no method works well in all cases. To address this limitation, we should avoid hand-designing task-specific state and action spaces and focus on ones that can be used widely. This could make data sharing easier, reduce the effort required to shift between different data distributions, and promote the development of methods that are less specialized.

By addressing these limitations, robots will hopefully be better at tasks like installing cables, sewing sutures, or using tools with pneumatic or hydraulic hoses. More broadly, the goal is to significantly improve robotic manipulation such that we can use it to replace work currently done by humans and do new work only suitable for robots. However, it is equally important to ensure this technology benefits society. There are significant risks of exacerbating wealth inequality and job displacement \cite{AutomationInequality, WEF2023}, and as scientists and engineers we must do our part to educate. This means educate others on what robots can do and how they work, but also educate ourselves on the impact these robots have on people and society. With this knowledge and mindset, I believe we can build a future where robotic manipulation is both capably and carefully deployed.

% Check all citations and add more!
